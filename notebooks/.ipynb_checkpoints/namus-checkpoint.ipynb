{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data, Scrape*\n",
    "\n",
    "Bryan Brugal\n",
    "\n",
    "PIT-DSC 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data from NamUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NamUs is the National Missing and Unidentified Persons System, which is financed by the United States Department of Justice. NamUs does not provide an API, however it does provide a searchable interface.\n",
    "\n",
    "In this notebook, we will use Selenium with beautifulSoup to retrieve and save data from NamUs as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import getopt, re, sys, time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize global driver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../web-scraping/namus/unclaimed_states_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Get path to the repository's data folder\n",
    "path = \"/\".join(os.getcwd().split(\"/\")[0:-1]) + \"../web-scraping/namus/unclaimed_states_combined.csv\"\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CASE_NUMBER_KEY = 'Case Number'\n",
    "\n",
    "INFO_COLUMNS = [\n",
    "    'Case Number',\n",
    "    'DBF',\n",
    "    'Last Name',\n",
    "    'First Name',\n",
    "    'Sex',\n",
    "    'Race/Ethnicity',\n",
    "    'City',\n",
    "    'County',\n",
    "    'State', \n",
    "    'Date Modified'\n",
    "]\n",
    "MAX_ROWS_PER_PAGE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_filters(filters):\n",
    "    if 'states' in filters: location_filter(filters['states'])\n",
    "    if 'date_elem' in filters: filtering_by_date(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "def filtering_by_date(date):\n",
    "    print('Adding date filters...')\n",
    "\n",
    "    section_on_circumstances = driver.find_element_by_id('Circumstances')\n",
    "    operand_box = section_on_circumstances.find_elements_by_tag_name('date-range-input')[1].find_elements_by_tag_name('select')[0]\n",
    "    Select(operand_box).select_by_visible_text(date['date_elem'])\n",
    "\n",
    "    time.sleep(.5)\n",
    "\n",
    "    month_box = section_on_circumstances.find_elements_by_tag_name('date-range-input')[1].find_elements_by_tag_name('select')[1]\n",
    "    Select(month_box).select_by_visible_text(date['month'])\n",
    "\n",
    "    day_box = section_on_circumstances.find_elements_by_tag_name('date-range-input')[1].find_elements_by_tag_name('select')[2]\n",
    "    Select(day_box).select_by_visible_text(date['day'])\n",
    "\n",
    "    year_box = section_on_circumstances.find_elements_by_tag_name('date-range-input')[1].find_elements_by_tag_name('select')[3]\n",
    "    Select(year_box).select_by_visible_text(date['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find state filter\n",
    "def location_filter(states):\n",
    "    print('Fetching selected states to filter...')\n",
    "    section_on_circumstances = driver.find_element_by_id('Circumstances')\n",
    "    labels_in_section = section_on_circumstances.find_elements_by_tag_name('label')\n",
    "\n",
    "    state_input_box = None\n",
    "\n",
    "    for label in labels_in_section:\n",
    "        if (label.text == \"State\"):\n",
    "            state_input_box = label.find_element_by_tag_name('input')\n",
    "            # add state filter\n",
    "            for state in states:\n",
    "                state_input_box.send_keys(state)\n",
    "                state_input_box.send_keys(Keys.ENTER)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records():\n",
    "    print('Collecting data...')\n",
    "\n",
    "    # navigate to list view\n",
    "    driver.find_element_by_xpath(\"//i[@class=\\\"icon-list\\\"]\").click()\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    df_info = pd.DataFrame(columns=INFO_COLUMNS)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    rows = soup.find('div', class_='ui-grid-canvas').contents\n",
    "\n",
    "    for row in rows:\n",
    "        if row != ' ':\n",
    "            cells = row.find_all('div', class_='ui-grid-cell-contents')\n",
    "            cells_text = map(lambda cell: cell.text.strip(), cells)\n",
    "            df_new_info = pd.DataFrame([list(cells_text)], columns=INFO_COLUMNS)\n",
    "            df_info = pd.concat([df_info, df_new_info], ignore_index=True)\n",
    "    \n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_counts():\n",
    "    print('Counting the amount of pages...')\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    page_num_data = soup.find('nav', {'aria-label': 'Page Selection'}).find('span').text\n",
    "    index_of_slash = re.search('/', page_num_data).span()[1]\n",
    "    page_numbers = int(page_num_data[index_of_slash:].strip())\n",
    "\n",
    "    return page_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page():\n",
    "    print('navigating to the next page...')\n",
    "    time.sleep(5)\n",
    "\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//i[@class=\\\"icon-triangle-right\\\"]\").click()\n",
    "    except:\n",
    "        print('last page completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(argv):\n",
    "    help_message = \"\"\"\n",
    "    Use: --states = New York\n",
    "        Allows a comma-separated list, such as (--states=Oregon, California).\n",
    "        Date of Last Interaction:-date= can search for dates greater or smaller than a certain date\n",
    "                    Example:    --date=\">=May-5-1995\" \n",
    "                                --date=\"<=February-12-1997\" \n",
    "        -h :        Displays a help screen; alternatively, use --help\n",
    "    \"\"\"\n",
    "\n",
    "    filters = {}\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,'h',['help', 'states=', 'date='])\n",
    "    except getopt.GetoptError:\n",
    "        print(help_message)\n",
    "        sys.exit(2)\n",
    "\n",
    "    for opt, arg in opts:\n",
    "        if opt in ('-h','--help'):\n",
    "            print(help_message)\n",
    "            sys.exit()\n",
    "        if opt == '--states':\n",
    "            filters['states'] = arg.split(',')\n",
    "        if opt == '--date':\n",
    "            filters['date_elem'] = arg[:2]\n",
    "            filters['month'] = arg[2:].split('-')[0]\n",
    "            filters['day'] = arg[2:].split('-')[1]\n",
    "            filters['year'] = arg[2:].split('-')[2]\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 100 results at a time\n",
    "def rows_to_show(num_rows):\n",
    "    print(f'Setting {MAX_ROWS_PER_PAGE} rows per page...')\n",
    "    dropdown_selection_results = driver.find_element_by_xpath(\"//label/span[contains(text(),'Results')]/following-sibling::select\")\n",
    "    Select(dropdown_selection_results).select_by_value(f'{num_rows}')\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    print('Searching...')\n",
    "    page_results = driver.find_element_by_class_name('search-criteria-container')\n",
    "    search_actions = page_results.find_element_by_class_name('search-criteria-container-actions').find_elements_by_tag_name('input')\n",
    "    search_actions[1].click()\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to namus.gov...\n",
      "Fetching selected states to filter...\n",
      "Searching...\n",
      "Starting case processing\n",
      "Setting 100 rows per page...\n",
      "Counting the amount of pages...\n",
      "Gathering page 0...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 1...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 2...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 3...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 4...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 5...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 6...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 7...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 8...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 9...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 10...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 11...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 12...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 13...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 14...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 15...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 16...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 17...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 18...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 19...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 20...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 21...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 22...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 23...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 24...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 25...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 26...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 27...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 28...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 29...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 30...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 31...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 32...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 33...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 34...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 35...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 36...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 37...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 38...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 39...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 40...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 41...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 42...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 43...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 44...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 45...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 46...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 47...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 48...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 49...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 50...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 51...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 52...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 53...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 54...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 55...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 56...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 57...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 58...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 59...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 60...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 61...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 62...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 63...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 64...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 65...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Gathering page 66...\n",
      "Collecting data...\n",
      "navigating to the next page...\n",
      "Saving gathered data to csv: ../web-scraping/namus/unclaimed_NY_states.csv\n",
      "Scraping completed\n"
     ]
    }
   ],
   "source": [
    "def main(argv):\n",
    "    filters = parse_args(\n",
    "        argv=['--states=New York']\n",
    "        )\n",
    "    \n",
    "    print('Navigating to namus.gov...')\n",
    "    driver.get(\"https://www.namus.gov/UnclaimedPersons/Search\")\n",
    "\n",
    "    add_filters(filters)\n",
    "    search()\n",
    "    print(\"Starting case processing\")\n",
    "\n",
    "    rows_to_show(MAX_ROWS_PER_PAGE)\n",
    "    page_numbers = page_counts()\n",
    "    df_info = pd.DataFrame(columns=INFO_COLUMNS)\n",
    "\n",
    "    try:\n",
    "        for page in range(page_numbers):\n",
    "            print(f'Gathering page {page}...')\n",
    "            new_df = records()\n",
    "            df_info = pd.concat([df_info, new_df], ignore_index=True)\n",
    "            next_page()\n",
    "    except Exception as e:\n",
    "        print(f'Exception thrown. Creating a csv file from existing data: {path}')\n",
    "        df_info.to_csv(path, index=False, encoding='utf-8')\n",
    "        #driver.quit()\n",
    "        print(e)\n",
    "    \n",
    "    # Output collected data to the \"web-scraping\" folder\n",
    "\n",
    "    print(f'Saving gathered data to csv: {path}')\n",
    "    df_info.to_csv(path, index=False, encoding='utf-8')\n",
    "    #driver.quit()\n",
    "\n",
    "    print('Scraping completed')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unclaimed = pd.read_csv('../web-scraping/namus/unclaimed_states.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>DBF</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race/Ethnicity</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Date Modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCP93119</td>\n",
       "      <td>07/05/2022</td>\n",
       "      <td>Jason</td>\n",
       "      <td>Jean</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Reno</td>\n",
       "      <td>Washoe</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>07/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCP93208</td>\n",
       "      <td>06/29/2022</td>\n",
       "      <td>Lehmann</td>\n",
       "      <td>Ursula</td>\n",
       "      <td>Female</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>Reno</td>\n",
       "      <td>Washoe</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>07/11/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCP93351</td>\n",
       "      <td>06/25/2022</td>\n",
       "      <td>Fowler</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Male</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>07/14/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCP93244</td>\n",
       "      <td>06/19/2022</td>\n",
       "      <td>Knutie</td>\n",
       "      <td>Christopher</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Knoxville</td>\n",
       "      <td>Knox</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>07/12/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCP93166</td>\n",
       "      <td>06/16/2022</td>\n",
       "      <td>Fisher</td>\n",
       "      <td>Laron</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Harris</td>\n",
       "      <td>Texas</td>\n",
       "      <td>07/08/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>UCP66090</td>\n",
       "      <td>--</td>\n",
       "      <td>Scripture</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Washington</td>\n",
       "      <td>04/04/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>UCP66093</td>\n",
       "      <td>--</td>\n",
       "      <td>Heaertel</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Washington</td>\n",
       "      <td>04/04/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>UCP66117</td>\n",
       "      <td>--</td>\n",
       "      <td>Jones</td>\n",
       "      <td>Lillian</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Washington</td>\n",
       "      <td>04/05/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>UCP66116</td>\n",
       "      <td>--</td>\n",
       "      <td>Welch</td>\n",
       "      <td>Lillian</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Washington</td>\n",
       "      <td>04/05/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>UCP66177</td>\n",
       "      <td>--</td>\n",
       "      <td>Horn</td>\n",
       "      <td>Kathleen</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>Washington</td>\n",
       "      <td>04/06/2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Case Number         DBF  Last Name   First Name     Sex  \\\n",
       "0       UCP93119  07/05/2022      Jason         Jean    Male   \n",
       "1       UCP93208  06/29/2022    Lehmann       Ursula  Female   \n",
       "2       UCP93351  06/25/2022     Fowler       Donald    Male   \n",
       "3       UCP93244  06/19/2022     Knutie  Christopher    Male   \n",
       "4       UCP93166  06/16/2022     Fisher        Laron    Male   \n",
       "...          ...         ...        ...          ...     ...   \n",
       "7498    UCP66090          --  Scripture    Elizabeth      --   \n",
       "7499    UCP66093          --   Heaertel       Jeremy      --   \n",
       "7500    UCP66117          --      Jones      Lillian      --   \n",
       "7501    UCP66116          --      Welch      Lillian      --   \n",
       "7502    UCP66177          --       Horn     Kathleen      --   \n",
       "\n",
       "                Race/Ethnicity        City     County       State  \\\n",
       "0     Black / African American        Reno     Washoe      Nevada   \n",
       "1            White / Caucasian        Reno     Washoe      Nevada   \n",
       "2            White / Caucasian  Birmingham  Jefferson     Alabama   \n",
       "3     Black / African American   Knoxville       Knox   Tennessee   \n",
       "4     Black / African American     Houston     Harris       Texas   \n",
       "...                        ...         ...        ...         ...   \n",
       "7498                        --          --     Yakima  Washington   \n",
       "7499                        --          --     Yakima  Washington   \n",
       "7500                        --          --     Yakima  Washington   \n",
       "7501                        --          --     Yakima  Washington   \n",
       "7502                        --          --     Yakima  Washington   \n",
       "\n",
       "     Date Modified  \n",
       "0       07/07/2022  \n",
       "1       07/11/2022  \n",
       "2       07/14/2022  \n",
       "3       07/12/2022  \n",
       "4       07/08/2022  \n",
       "...            ...  \n",
       "7498    04/04/2020  \n",
       "7499    04/04/2020  \n",
       "7500    04/05/2020  \n",
       "7501    04/05/2020  \n",
       "7502    04/06/2020  \n",
       "\n",
       "[7503 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unclaimed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unclaimed_NY = pd.read_csv('../web-scraping/namus/unclaimed_NY_states.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Data...\n"
     ]
    }
   ],
   "source": [
    "print('Merging Data...')\n",
    "# Append two pandas DataFrames\n",
    "merged_df = pd.concat([df_unclaimed, df_unclaimed_NY],  ignore_index = True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>DBF</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race/Ethnicity</th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Date Modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCP93119</td>\n",
       "      <td>07/05/2022</td>\n",
       "      <td>Jason</td>\n",
       "      <td>Jean</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Reno</td>\n",
       "      <td>Washoe</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>07/07/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCP93208</td>\n",
       "      <td>06/29/2022</td>\n",
       "      <td>Lehmann</td>\n",
       "      <td>Ursula</td>\n",
       "      <td>Female</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>Reno</td>\n",
       "      <td>Washoe</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>07/11/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCP93351</td>\n",
       "      <td>06/25/2022</td>\n",
       "      <td>Fowler</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Male</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>07/14/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCP93244</td>\n",
       "      <td>06/19/2022</td>\n",
       "      <td>Knutie</td>\n",
       "      <td>Christopher</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Knoxville</td>\n",
       "      <td>Knox</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>07/12/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCP93166</td>\n",
       "      <td>06/16/2022</td>\n",
       "      <td>Fisher</td>\n",
       "      <td>Laron</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Harris</td>\n",
       "      <td>Texas</td>\n",
       "      <td>07/08/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14101</th>\n",
       "      <td>UCP2094</td>\n",
       "      <td>--</td>\n",
       "      <td>Hill</td>\n",
       "      <td>Eugene</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>03/29/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14102</th>\n",
       "      <td>UCP2106</td>\n",
       "      <td>--</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black / African American</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>New York</td>\n",
       "      <td>03/29/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14103</th>\n",
       "      <td>UCP2683</td>\n",
       "      <td>--</td>\n",
       "      <td>Perez</td>\n",
       "      <td>Carmelino</td>\n",
       "      <td>Male</td>\n",
       "      <td>Uncertain</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>03/29/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14104</th>\n",
       "      <td>UCP5194</td>\n",
       "      <td>--</td>\n",
       "      <td>Faeth</td>\n",
       "      <td>Gerald</td>\n",
       "      <td>Male</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>05/07/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14105</th>\n",
       "      <td>UCP5055</td>\n",
       "      <td>--</td>\n",
       "      <td>Patel</td>\n",
       "      <td>Nareshkumar</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kings</td>\n",
       "      <td>New York</td>\n",
       "      <td>05/07/2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14106 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Case Number         DBF Last Name   First Name     Sex  \\\n",
       "0        UCP93119  07/05/2022     Jason         Jean    Male   \n",
       "1        UCP93208  06/29/2022   Lehmann       Ursula  Female   \n",
       "2        UCP93351  06/25/2022    Fowler       Donald    Male   \n",
       "3        UCP93244  06/19/2022    Knutie  Christopher    Male   \n",
       "4        UCP93166  06/16/2022    Fisher        Laron    Male   \n",
       "...           ...         ...       ...          ...     ...   \n",
       "14101     UCP2094          --      Hill       Eugene    Male   \n",
       "14102     UCP2106          --   Germany        Kevin    Male   \n",
       "14103     UCP2683          --     Perez    Carmelino    Male   \n",
       "14104     UCP5194          --     Faeth       Gerald    Male   \n",
       "14105     UCP5055          --     Patel  Nareshkumar    Male   \n",
       "\n",
       "                 Race/Ethnicity        City     County      State  \\\n",
       "0      Black / African American        Reno     Washoe     Nevada   \n",
       "1             White / Caucasian        Reno     Washoe     Nevada   \n",
       "2             White / Caucasian  Birmingham  Jefferson    Alabama   \n",
       "3      Black / African American   Knoxville       Knox  Tennessee   \n",
       "4      Black / African American     Houston     Harris      Texas   \n",
       "...                         ...         ...        ...        ...   \n",
       "14101  Black / African American    New York   New York   New York   \n",
       "14102  Black / African American      Queens     Queens   New York   \n",
       "14103                 Uncertain    New York   New York   New York   \n",
       "14104         White / Caucasian    New York   New York   New York   \n",
       "14105                     Asian    Brooklyn      Kings   New York   \n",
       "\n",
       "      Date Modified  \n",
       "0        07/07/2022  \n",
       "1        07/11/2022  \n",
       "2        07/14/2022  \n",
       "3        07/12/2022  \n",
       "4        07/08/2022  \n",
       "...             ...  \n",
       "14101    03/29/2021  \n",
       "14102    03/29/2021  \n",
       "14103    03/29/2021  \n",
       "14104    05/07/2020  \n",
       "14105    05/07/2020  \n",
       "\n",
       "[14106 rows x 10 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display append dataframe\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(path, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "396ec3599db068b730f3c31fcff11d8f15ee156b078a9c463ed3d59b4ce6130a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
